{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novelty and outlier detection\n",
    "\n",
    "Going through all of the `scikit-learn` methods for outlier detection to see what we get!\n",
    "\n",
    "## Data\n",
    "\n",
    "Import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I deleted three of the missingness indicators (the ones for the variables that are very rarely missing). I also deleted the source column, which is TELEAPP only 0.71% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned data from clean_data.py\n",
    "df = pd.read_csv('../data/cleaned_Base.csv')\n",
    "\n",
    "target = 'fraud_bool'\n",
    "X = df.drop(target, axis = 1)\n",
    "y = df[target]\n",
    "\n",
    "# Drop columns we don't want in the model\n",
    "# Missing flags that the Kaggle guy doesn't use\n",
    "# source\n",
    "cols_to_drop = ['current_address_months_count_ismissing',\n",
    "                'session_length_in_minutes_ismissing',\n",
    "                'device_distinct_emails_8w_ismissing',\n",
    "                'source']\n",
    "X = X.drop(cols_to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                               int64\n",
      "income                                 float64\n",
      "name_email_similarity                  float64\n",
      "current_address_months_count             int64\n",
      "customer_age                             int64\n",
      "days_since_request                     float64\n",
      "payment_type                            object\n",
      "zip_count_4w                             int64\n",
      "velocity_6h                            float64\n",
      "velocity_24h                           float64\n",
      "velocity_4w                            float64\n",
      "bank_branch_count_8w                     int64\n",
      "date_of_birth_distinct_emails_4w         int64\n",
      "employment_status                       object\n",
      "credit_risk_score                        int64\n",
      "email_is_free                            int64\n",
      "housing_status                          object\n",
      "phone_home_valid                         int64\n",
      "phone_mobile_valid                       int64\n",
      "has_other_cards                          int64\n",
      "proposed_credit_limit                  float64\n",
      "foreign_request                          int64\n",
      "session_length_in_minutes              float64\n",
      "device_os                               object\n",
      "keep_alive_session                       int64\n",
      "device_distinct_emails_8w                int64\n",
      "month                                    int64\n",
      "prev_address_months_count_ismissing      int64\n",
      "bank_months_count_ismissing              int64\n",
      "intended_balcon_amount_ismissing         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print all column names and classes of X\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X['device_distinct_emails_8w'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I one-hot encoded 4 of the variables.\n",
    "\n",
    "For the training set, I filtered to month 0, and then tested on month 1. Based on this, I deleted the month columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132440, 49)\n"
     ]
    }
   ],
   "source": [
    "# Create one-hot encoded version\n",
    "ohe_cols = ['payment_type', \n",
    "            'employment_status', \n",
    "            'housing_status', \n",
    "            # 'source',\n",
    "            'device_os']\n",
    "\n",
    "X_ohe = pd.get_dummies(X, columns=ohe_cols)\n",
    "\n",
    "# Train on month 0\n",
    "X_ohe_train = X_ohe.loc[X_ohe['month'] == 0]\n",
    "X_ohe_train = X_ohe_train.drop('month', axis = 1)\n",
    "print(X_ohe_train.shape)\n",
    "y_train = y[X_ohe['month'] == 0]\n",
    "\n",
    "# Test on month 1\n",
    "X_ohe_test = X_ohe.loc[X_ohe['month'] == 1]\n",
    "X_ohe_test = X_ohe_test.drop('month', axis = 1)\n",
    "y_test = y[X_ohe['month'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also created scaled versions of the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaled version\n",
    "scaler = StandardScaler()\n",
    "X_ohe_train_scaled = scaler.fit_transform(X_ohe_train)\n",
    "X_ohe_test_scaled = scaler.fit_transform(X_ohe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some functions to make the metrics nice (stolen from https://github.com/southworks/machine-learning-unsupervised/blob/main/anomaly-detection/fraud_detection.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup functions\n",
    "# Create a results dataframe to store and later compare results\n",
    "results = pd.DataFrame()\n",
    "results['Metrics'] = ['True Negatives', 'False Negatives', 'False Positives', 'True Positives', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
    "\n",
    "def get_metrics(labels, pred_labels):\n",
    "    conf_matrix = np.asarray(confusion_matrix(labels, pred_labels))\n",
    "    metrics = np.concatenate(([accuracy_score(labels, pred_labels)], precision_recall_fscore_support(labels, pred_labels, average='binary')))\n",
    "    return np.concatenate((conf_matrix.reshape(-1),metrics))\n",
    "\n",
    "def fit_model(input_data, model_fn, model_args=None, threshold=None, labels=None):\n",
    "    return model_fn(input_data, model_args, threshold, labels)\n",
    "\n",
    "def predict_model(input_data, model, predict_fn, threshold=None):\n",
    "    return predict_fn(input_data, model, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation forest\n",
    "\n",
    "Uses decision trees: randomly selects a feature and then randomly selects a split value. Theoretically, outliers will need fewer random partitions to be isolated and thus have a shorter path length on the tree.\n",
    "\n",
    "- Scale: no\n",
    "- One-hot encode: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_isolation_forest(input_data, args=None, threshold=None, labels=None):\n",
    "    if threshold == None:\n",
    "        model = IsolationForest()\n",
    "    else:\n",
    "        model = IsolationForest(contamination=threshold)\n",
    "    \n",
    "    model.fit(input_data)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_isolation_forest(input_data, model, threshold=None):\n",
    "    y_pred = model.predict(input_data)\n",
    "    y_pred = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of fraud in the entire dataset:  0.011029\n"
     ]
    }
   ],
   "source": [
    "# Calculate the contamination parameter\n",
    "contamination_rate = y.sum() / len(y) # going with overall incidence in the data\n",
    "print(\"Proportion of fraud in the entire dataset: \", contamination_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      " [[123927   2495]\n",
      " [  1125     73]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99    126422\n",
      "           1       0.03      0.06      0.04      1198\n",
      "\n",
      "    accuracy                           0.97    127620\n",
      "   macro avg       0.51      0.52      0.51    127620\n",
      "weighted avg       0.98      0.97      0.98    127620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run isolation forest on training data\n",
    "if_model = fit_model(X_ohe_train, fit_isolation_forest, threshold=contamination_rate)\n",
    "\n",
    "if_pred_test = predict_model(X_ohe_test, if_model, predict_isolation_forest)\n",
    "\n",
    "print('CONFUSION MATRIX:\\n', confusion_matrix(y_test,if_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, if_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Isolation Forest'] = get_metrics(y_test, if_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Outlier Factor (LOF)\n",
    "\n",
    "LOF computes the local density deviation of a given data point with respect to its neighbors. If a point has substantially lower density than its neighbors, it is identified as an outlier.\n",
    "\n",
    "The contamination parameter sets the proportion of the most isolated points to be predicted as anomalies, so we want to set that to what we believe the incidence of fraud to be.\n",
    "\n",
    "- Scale: I think so? Gonna do it\n",
    "- One-hot encode: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOF model\n",
    "lof_model = LocalOutlierFactor(n_neighbors = 200, # heuristic\n",
    "                               contamination = contamination_rate,\n",
    "                               novelty=False)\n",
    "# Need to just run it on the testing data and identify the outliers there\n",
    "lof_pred_test = lof_model.fit_predict(X_ohe_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1407.52098"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ohe_test_scaled.shape[0] * contamination_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION RESULTS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    126422\n",
      "           1       0.02      0.02      0.02      1198\n",
      "\n",
      "    accuracy                           0.98    127620\n",
      "   macro avg       0.51      0.51      0.51    127620\n",
      "weighted avg       0.98      0.98      0.98    127620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# np.unique(lof_pred_train, return_counts = True)\n",
    "# Recode the predictions\n",
    "lof_pred_test[lof_pred_test == 1] = 0\n",
    "lof_pred_test[lof_pred_test == -1] = 1\n",
    "\n",
    "print('CLASSIFICATION RESULTS')\n",
    "print(classification_report(y_test, lof_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Isolation Forest</th>\n",
       "      <th>LOF</th>\n",
       "      <th>Gaussian Mixture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True Negatives</td>\n",
       "      <td>123927</td>\n",
       "      <td>125042</td>\n",
       "      <td>125025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False Negatives</td>\n",
       "      <td>2495</td>\n",
       "      <td>1380</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False Positives</td>\n",
       "      <td>1125</td>\n",
       "      <td>1170</td>\n",
       "      <td>1187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True Positives</td>\n",
       "      <td>73</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.971635</td>\n",
       "      <td>0.980019</td>\n",
       "      <td>0.979752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.028427</td>\n",
       "      <td>0.019886</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.060935</td>\n",
       "      <td>0.023372</td>\n",
       "      <td>0.009182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1-Score</td>\n",
       "      <td>0.038768</td>\n",
       "      <td>0.021489</td>\n",
       "      <td>0.008442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Metrics Isolation Forest       LOF Gaussian Mixture\n",
       "0   True Negatives           123927    125042           125025\n",
       "1  False Negatives             2495      1380             1397\n",
       "2  False Positives             1125      1170             1187\n",
       "3   True Positives               73        28               11\n",
       "4         Accuracy         0.971635  0.980019         0.979752\n",
       "5        Precision         0.028427  0.019886         0.007812\n",
       "6           Recall         0.060935  0.023372         0.009182\n",
       "7         F1-Score         0.038768  0.021489         0.008442\n",
       "8          Support             None      None             None"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['LOF'] = get_metrics(y_test, lof_pred_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture\n",
    "\n",
    "Gaussian mixture models assume all data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. They try to find different subpopulations within the overall dataset.\n",
    "\n",
    "- Scale: eh why not\n",
    "- One-hot encode: yea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_gaussian_mixture(input_data, args=None, threshold=None, labels=None):\n",
    "#     if threshold == None:\n",
    "#         model = GaussianMixture(n_components=2)\n",
    "#     else:\n",
    "#         model = GaussianMixture(n_components=2, reg_covar=threshold)\n",
    "    \n",
    "#     model.fit(input_data)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# def predict_gaussian_mixture(input_data, model, threshold=None):\n",
    "#     return model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      " [[125025   1397]\n",
      " [  1187     11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    126422\n",
      "           1       0.01      0.01      0.01      1198\n",
      "\n",
      "    accuracy                           0.98    127620\n",
      "   macro avg       0.50      0.50      0.50    127620\n",
      "weighted avg       0.98      0.98      0.98    127620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm = GaussianMixture(n_components=1)\n",
    "gmm.fit(X_ohe_train_scaled)\n",
    "\n",
    "# Compute log-likelihood on test data\n",
    "gm_loglikelihood = gmm.score_samples(X_ohe_test_scaled)\n",
    "\n",
    "# Set threshold\n",
    "ll_threshold = np.percentile(gm_loglikelihood, 100 * contamination_rate)\n",
    "\n",
    "gm_pred_test = gm_loglikelihood < ll_threshold\n",
    "print('CONFUSION MATRIX:\\n', confusion_matrix(y_test,gm_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, gm_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Isolation Forest</th>\n",
       "      <th>LOF</th>\n",
       "      <th>Gaussian Mixture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True Negatives</td>\n",
       "      <td>123927</td>\n",
       "      <td>125038</td>\n",
       "      <td>125025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False Negatives</td>\n",
       "      <td>2495</td>\n",
       "      <td>1384</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False Positives</td>\n",
       "      <td>1125</td>\n",
       "      <td>1174</td>\n",
       "      <td>1187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True Positives</td>\n",
       "      <td>73</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.971635</td>\n",
       "      <td>0.979956</td>\n",
       "      <td>0.979752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.028427</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.060935</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.009182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1-Score</td>\n",
       "      <td>0.038768</td>\n",
       "      <td>0.018419</td>\n",
       "      <td>0.008442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Metrics Isolation Forest       LOF Gaussian Mixture\n",
       "0   True Negatives           123927    125038           125025\n",
       "1  False Negatives             2495      1384             1397\n",
       "2  False Positives             1125      1174             1187\n",
       "3   True Positives               73        24               11\n",
       "4         Accuracy         0.971635  0.979956         0.979752\n",
       "5        Precision         0.028427  0.017045         0.007812\n",
       "6           Recall         0.060935  0.020033         0.009182\n",
       "7         F1-Score         0.038768  0.018419         0.008442\n",
       "8          Support             None      None             None"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Gaussian Mixture'] = get_metrics(y_test, gm_pred_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
